{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14246478,"sourceType":"datasetVersion","datasetId":9089436}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport os\nfrom collections import Counter\nimport cv2\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport torch\nfrom PIL import Image \nfrom torch.utils.data.dataset import Dataset\nfrom torchvision import transforms\nfrom torch import optim , nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport os\nimport numpy as np\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:06:41.874171Z","iopub.execute_input":"2025-12-27T12:06:41.874567Z","iopub.status.idle":"2025-12-27T12:06:56.245699Z","shell.execute_reply.started":"2025-12-27T12:06:41.874535Z","shell.execute_reply":"2025-12-27T12:06:56.244620Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"LEARNING_RATE=3e-4\nBATCH_SIZE=5\nEPOCHS=30\nDATA_PATH=\"/kaggle/input/smoke-segmentation/ALL-2\"\nMODEL_SAVE_PATH = \"/kaggle/working/unet_smoke_model.pth\"\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:06:56.247850Z","iopub.execute_input":"2025-12-27T12:06:56.248379Z","iopub.status.idle":"2025-12-27T12:06:56.257326Z","shell.execute_reply.started":"2025-12-27T12:06:56.248347Z","shell.execute_reply":"2025-12-27T12:06:56.255802Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def check_proportions():\n    # Count images in each folder\n    train_imgs = len([f for f in os.listdir(os.path.join(DATA_PATH, 'train')) if f.endswith('.jpg')])\n    valid_imgs = len([f for f in os.listdir(os.path.join(DATA_PATH, 'valid')) if f.endswith('.jpg')])\n    test_imgs  = len([f for f in os.listdir(os.path.join(DATA_PATH, 'test'))  if f.endswith('.jpg')])\n    \n    total = train_imgs + valid_imgs + test_imgs\n    \n    print(f\"Train: {train_imgs} images ({train_imgs/total*100:.1f}%)\")\n    print(f\"Valid: {valid_imgs} images ({valid_imgs/total*100:.1f}%)\")\n    print(f\"Test:  {test_imgs} images ({test_imgs/total*100:.1f}%)\")\n    print(f\"Total: {total} images\")\n    \n    if 0.60 <= train_imgs/total <= 0.85:\n        print(\"Proportion train OK (60-85%)\")\n    else:\n        print(\"Proportion train inhabituelle\")\n\n# Call the function\ncheck_proportions()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:31:10.550484Z","iopub.execute_input":"2025-12-27T12:31:10.550815Z","iopub.status.idle":"2025-12-27T12:31:10.564425Z","shell.execute_reply.started":"2025-12-27T12:31:10.550787Z","shell.execute_reply":"2025-12-27T12:31:10.563342Z"}},"outputs":[{"name":"stdout","text":"Train: 400 images (74.5%)\nValid: 40 images (7.4%)\nTest:  97 images (18.1%)\nTotal: 537 images\nProportion train OK (60-85%)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def check_dimensions_match(folder='train', num_samples=50):\n    folder_path = os.path.join(DATA_PATH, folder)\n    images = [f for f in sorted(os.listdir(folder_path)) if f.endswith('.jpg')][:num_samples]\n    \n    print(f\"\\n=== DIMENSIONS {folder.upper()} ===\")\n    \n    mismatches = []\n    for img_file in images:\n        img_path = os.path.join(folder_path, img_file)\n        mask_file = img_file.replace('.jpg', '_mask.png')\n        mask_path = os.path.join(folder_path, mask_file)\n        \n        img = cv2.imread(img_path)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        \n        if img is None:\n            print(f\"❌ Image corrompue: {img_file}\")\n            continue\n        if mask is None:\n            print(f\"❌ Masque corrompu: {mask_file}\")\n            continue\n        \n        if img.shape[:2] != mask.shape:\n            print(f\"❌ {img_file}: Image {img.shape[:2]} ≠ Masque {mask.shape}\")\n            mismatches.append(img_file)\n    \n    if not mismatches:\n        print(f\"✅ Toutes les dimensions correspondent!\")\n        if images:\n            img = cv2.imread(os.path.join(folder_path, images[0]))\n            print(f\"   Taille: {img.shape[:2]} (H, W)\")\n    \n    return len(mismatches) == 0\n\n# Run checks\ncheck_dimensions_match('train')\ncheck_dimensions_match('test')\ncheck_dimensions_match('valid')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:32:36.430849Z","iopub.execute_input":"2025-12-27T12:32:36.431290Z","iopub.status.idle":"2025-12-27T12:32:39.676737Z","shell.execute_reply.started":"2025-12-27T12:32:36.431259Z","shell.execute_reply":"2025-12-27T12:32:39.675884Z"}},"outputs":[{"name":"stdout","text":"\n=== DIMENSIONS TRAIN ===\n✅ Toutes les dimensions correspondent!\n   Taille: (640, 640) (H, W)\n\n=== DIMENSIONS TEST ===\n✅ Toutes les dimensions correspondent!\n   Taille: (640, 640) (H, W)\n\n=== DIMENSIONS VALID ===\n✅ Toutes les dimensions correspondent!\n   Taille: (480, 852) (H, W)\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def analyze_classes(folder='train'):\n    folder_path = os.path.join(DATA_PATH, folder)\n    masks = [f for f in sorted(os.listdir(folder_path)) if f.endswith('.png')]\n    \n    pixel_counts = Counter()\n    all_classes = set()\n    \n    print(f\"\\n=== CLASSES {folder.upper()} ===\")\n    \n    for mask_file in masks:\n        mask = cv2.imread(f'/kaggle/input/smoke-segmentation/ALL-2/{folder}/{mask_file}', cv2.IMREAD_GRAYSCALE)\n        unique, counts = np.unique(mask, return_counts=True)\n        \n        all_classes.update(unique)\n        for cls, count in zip(unique, counts):\n            pixel_counts[cls] += count\n    \n    print(f\"Classes trouvées: {sorted(all_classes)}\")\n    \n    total_pixels = sum(pixel_counts.values())\n    print(f\"\\nDistribution:\")\n    for cls in sorted(pixel_counts.keys()):\n        count = pixel_counts[cls]\n        percentage = count / total_pixels * 100\n        class_name = \"Background\" if cls == 0 else f\"Smoke (classe {cls})\"\n        print(f\"  {class_name}: {count:,} pixels ({percentage:.2f}%)\")\n    \n    # Vérifier déséquilibre (sans compter le background)\n    non_bg_counts = {k: v for k, v in pixel_counts.items() if k != 0}\n    if len(non_bg_counts) > 0:\n        smoke_pixels = sum(non_bg_counts.values())\n        bg_pixels = pixel_counts[0]\n        ratio = bg_pixels / smoke_pixels if smoke_pixels > 0 else 0\n        \n        print(f\"\\nRatio Background/Fumée: {ratio:.1f}:1\")\n        \n        if ratio > 100:\n            print(\"Fort déséquilibre - considérez les poids de classe\")\n        else:\n            print(\"Déséquilibre acceptable\")\n    \n    return pixel_counts\n\nanalyze_classes('train')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:35:06.101464Z","iopub.execute_input":"2025-12-27T12:35:06.101811Z","iopub.status.idle":"2025-12-27T12:35:16.247549Z","shell.execute_reply.started":"2025-12-27T12:35:06.101783Z","shell.execute_reply":"2025-12-27T12:35:16.246664Z"}},"outputs":[{"name":"stdout","text":"\n=== CLASSES TRAIN ===\nClasses trouvées: [np.uint8(0), np.uint8(1), np.uint8(2), np.uint8(3)]\n\nDistribution:\n  Background: 315,254,880 pixels (90.52%)\n  Smoke (classe 1): 864 pixels (0.00%)\n  Smoke (classe 2): 5,438 pixels (0.00%)\n  Smoke (classe 3): 33,020,553 pixels (9.48%)\n\nRatio Background/Fumée: 9.5:1\n✅ Déséquilibre acceptable\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Counter({np.uint8(0): np.int64(315254880),\n         np.uint8(3): np.int64(33020553),\n         np.uint8(2): np.int64(5438),\n         np.uint8(1): np.int64(864)})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"def check_empty_masks(folder='train'):\n    folder_path = os.path.join(DATA_PATH, folder)\n    masks = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n    \n    empty_masks = []\n    \n    print(f\"\\n=== MASQUES VIDES {folder.upper()} ===\")\n    \n    for mask_file in masks:\n        mask_path = os.path.join(folder_path, mask_file)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        \n        if mask is None:\n            print(f\"Masque corrompu: {mask_file}\")\n            continue\n        \n        unique = np.unique(mask)\n        \n        # Si seulement background (classe 0)\n        if len(unique) == 1 and unique[0] == 0:\n            empty_masks.append(mask_file)\n    \n    print(f\"Masques vides: {len(empty_masks)}/{len(masks)}\")\n    \n    if empty_masks:\n        print(f\"{len(empty_masks)} masques sans fumée\")\n        print(f\"   Exemples: {empty_masks[:5]}\")\n        \n        # Vérifier si proportion élevée\n        percentage = len(empty_masks) / len(masks) * 100\n        if percentage > 10:\n            print(f\"{percentage:.1f}% de masques vides - est-ce normal?\")\n    else:\n        print(\"✅ Tous les masques contiennent de la fumée\")\n    \n    return empty_masks\n\n# Exemple d'appel\ncheck_empty_masks('train')\ncheck_empty_masks('valid')\ncheck_empty_masks('test')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:37:55.366529Z","iopub.execute_input":"2025-12-27T12:37:55.366861Z","iopub.status.idle":"2025-12-27T12:38:06.816635Z","shell.execute_reply.started":"2025-12-27T12:37:55.366835Z","shell.execute_reply":"2025-12-27T12:38:06.815415Z"}},"outputs":[{"name":"stdout","text":"\n=== MASQUES VIDES TRAIN ===\nMasques vides: 0/400\n✅ Tous les masques contiennent de la fumée\n\n=== MASQUES VIDES VALID ===\nMasques vides: 0/40\n✅ Tous les masques contiennent de la fumée\n\n=== MASQUES VIDES TEST ===\nMasques vides: 0/97\n✅ Tous les masques contiennent de la fumée\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:06:56.308710Z","iopub.status.idle":"2025-12-27T12:06:56.309306Z","shell.execute_reply.started":"2025-12-27T12:06:56.309083Z","shell.execute_reply":"2025-12-27T12:06:56.309115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:06:56.310496Z","iopub.status.idle":"2025-12-27T12:06:56.310884Z","shell.execute_reply.started":"2025-12-27T12:06:56.310694Z","shell.execute_reply":"2025-12-27T12:06:56.310720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SmokeDataset(Dataset):\n    def __init__(self, root_path, folder, transforms=None):\n        self.root_path = root_path\n        self.images = sorted([\n            os.path.join(root_path, folder, i)\n            for i in os.listdir(os.path.join(root_path, folder))\n            if i.endswith(\".jpg\")\n        ])\n        self.masks = sorted([\n            os.path.join(root_path, folder, i)\n            for i in os.listdir(os.path.join(root_path, folder))\n            if i.endswith(\".png\")\n        ])\n        self.transforms = transforms\n\n    def __getitem__(self, index):\n        image = Image.open(self.images[index]).convert('RGB')\n        mask = Image.open(self.masks[index]).convert('L')\n        \n        image = image.resize((512, 512))\n        mask = mask.resize((512, 512))\n        \n        img = np.array(image)\n        mask = np.array(mask)\n        \n        mask[mask > 0] = 1\n        \n        if self.transforms:\n            augmented = self.transforms(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n    \n        return img, mask\n    \n\n    def __len__(self):\n        return len(self.images)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:06:56.313243Z","iopub.status.idle":"2025-12-27T12:06:56.313579Z","shell.execute_reply.started":"2025-12-27T12:06:56.313432Z","shell.execute_reply":"2025-12-27T12:06:56.313451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_transforms = A.Compose([\n    A.Resize(512, 512),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, p=0.5),\n    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n    ToTensorV2()\n])\n\nval_transforms = A.Compose([\n    A.Resize(512, 512),\n    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n    ToTensorV2()\n])\n\ntrain_dataset = SmokeDataset(DATA_PATH, 'train', transforms=train_transforms)\nval_dataset = SmokeDataset(DATA_PATH, 'valid', transforms=val_transforms)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:06:56.316060Z","iopub.status.idle":"2025-12-27T12:06:56.316532Z","shell.execute_reply.started":"2025-12-27T12:06:56.316375Z","shell.execute_reply":"2025-12-27T12:06:56.316396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv_op = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels), \n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.conv_op(x)\n\nclass DownSample(nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super().__init__()\n        self.conv=DoubleConv(in_channels,out_channels)\n        self.pool=nn.MaxPool2d(kernel_size=2,stride=2)\n    def forward(self,x):\n        down=self.conv(x)\n        p=self.pool(down)\n        return down,p\nclass UpSample(nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super().__init__()\n        self.up=nn.ConvTranspose2d(in_channels,in_channels//2,kernel_size=2,stride=2)\n        self.conv=DoubleConv(in_channels,out_channels)\n    def forward(self,x1,x2):\n        x1=self.up(x1)\n        x=torch.cat([x1,x2],1)\n        return self.conv(x)\nclass UNet(nn.Module):\n    def __init__(self,in_channels,num_classes):\n        super().__init__()\n        self.down_convolution_1=DownSample(in_channels,64)\n        self.down_convolution_2=DownSample(64,128)\n        self.down_convolution_3=DownSample(128,256)\n        self.down_convolution_4=DownSample(256,512)\n        \n        self.bottle_neck=DoubleConv(512,1024)\n\n        self.up_convolution_1=UpSample(1024,512)\n        self.up_convolution_2=UpSample(512,256)\n        self.up_convolution_3=UpSample(256,128)\n        self.up_convolution_4=UpSample(128,64)\n\n\n        self.out=nn.Conv2d(in_channels=64,out_channels=num_classes,kernel_size=1)\n    def forward(self,x):\n        down_1,p1=self.down_convolution_1(x)\n        down_2,p2=self.down_convolution_2(p1)\n        down_3,p3=self.down_convolution_3(p2)\n        down_4,p4=self.down_convolution_4(p3)\n        \n        b=self.bottle_neck(p4)\n\n        up_1=self.up_convolution_1(b,down_4)\n        up_2=self.up_convolution_2(up_1,down_3)\n        up_3=self.up_convolution_3(up_2,down_2)\n        up_4=self.up_convolution_4(up_3,down_1)\n        out=self.out(up_4)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:06:56.318753Z","iopub.status.idle":"2025-12-27T12:06:56.319191Z","shell.execute_reply.started":"2025-12-27T12:06:56.318997Z","shell.execute_reply":"2025-12-27T12:06:56.319023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_coef(y_pred, y_true, smooth=1e-6):\n    y_pred = torch.sigmoid(y_pred)  # convert logits to probabilities\n    y_true = y_true.float()\n    intersection = (y_pred * y_true).sum(dim=(2,3))\n    union = y_pred.sum(dim=(2,3)) + y_true.sum(dim=(2,3))\n    dice = ((2 * intersection + smooth) / (union + smooth)).mean()\n    return dice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:06:56.321071Z","iopub.status.idle":"2025-12-27T12:06:56.321429Z","shell.execute_reply.started":"2025-12-27T12:06:56.321265Z","shell.execute_reply":"2025-12-27T12:06:56.321284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = UNet(in_channels=3, num_classes=1).to(device)\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.BCEWithLogitsLoss()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_running_loss = 0\n    train_running_dice = 0\n\n    for idx, (img, mask) in enumerate(train_dataloader):\n        img = img.float().to(device)\n        mask = mask.float().to(device)\n        mask[mask > 0] = 1\n        mask = mask.unsqueeze(1)  # (N,1,H,W)\n\n        optimizer.zero_grad()\n        y_pred = model(img)\n\n        # BCE used for backprop\n        loss = criterion(y_pred, mask)\n        loss.backward()\n        optimizer.step()\n\n        train_running_loss += loss.item()\n        # Compute Dice as metric (no backward)\n        train_running_dice += dice_coef(y_pred, mask).item()\n\n    train_loss = train_running_loss / (idx + 1)\n    train_dice = train_running_dice / (idx + 1)\n\n    # Validation\n    model.eval()\n    val_running_loss = 0\n    val_running_dice = 0\n    with torch.no_grad():\n        for idx, (img, mask) in enumerate(val_dataloader):\n            img = img.float().to(device)\n            mask = mask.float().to(device)\n            mask = mask.unsqueeze(1)\n\n            y_pred = model(img)\n            loss = criterion(y_pred, mask)\n\n            val_running_loss += loss.item()\n            val_running_dice += dice_coef(y_pred, mask).item()\n\n    val_loss = val_running_loss / (idx + 1)\n    val_dice = val_running_dice / (idx + 1)\n\n    print('-'*30)\n    print(f'Train Loss: {train_loss:.4f} | Train Dice: {train_dice:.4f}')\n    print(f'Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}')\n\ntorch.save(model.state_dict(), MODEL_SAVE_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:06:56.323606Z","iopub.status.idle":"2025-12-27T12:06:56.324272Z","shell.execute_reply.started":"2025-12-27T12:06:56.324018Z","shell.execute_reply":"2025-12-27T12:06:56.324053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}